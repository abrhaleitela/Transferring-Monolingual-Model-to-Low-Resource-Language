{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "token_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDzMXDgnBMgv",
        "colab_type": "text"
      },
      "source": [
        "Train language dependent word2vec token embeddings \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayiaJvVCCU2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sentencepiece as spm\n",
        "from transformers import XLNetTokenizer\n",
        "from gensim.models import Word2Vec,word2vec\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import nltk\n",
        "import logging\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dPufZn1ORo-i",
        "colab": {}
      },
      "source": [
        "#pre-process the given text and return array of pure sentences\n",
        "def normalize_text(path):\n",
        "  normalized_text = []\n",
        "  with open(path, 'r', encoding ='utf-8') as file:\n",
        "    text_data = file.readlines()\n",
        "    for line in text_data:\n",
        "      if len(line) > 2:\n",
        "        regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "        line = \" \".join(regex_tokenizer.tokenize(line))\n",
        "        normalized_text.append(line)\n",
        "    return normalized_text  \n",
        "\n",
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "def read_sentences(f_path):\n",
        "  with open(f_path,'r',encoding = 'utf-8') as f:\n",
        "    dataset = f.readlines()\n",
        "  return dataset\n",
        "\n",
        "def store(comments, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(comments))\n",
        "\n",
        "def train_embedding(model_path, sentences, dimension, window, min_count,iter):\n",
        "    model = word2vec.Word2Vec(sentences, size=dimension, window=window, min_count=min_count, max_vocab_size=32000,iter=iter)\n",
        "    model.save(model_path) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vk_tWB6CBVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = 'data/pure_text_for_embed.txt'\n",
        "normalized_text = normalize_text(dataset_path)\n",
        "store(normalized_text, 'dataset.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wCGf45roSKqM",
        "outputId": "cbab197e-e484-4904-bbf9-b96c14bd24f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('tokenizer.model', do_lower_case=True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent.rstrip('\\n')) for sent in normalized_text]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling XLNetTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nG0S3M9dczIr",
        "colab": {}
      },
      "source": [
        "model_path_fast =\"data/fasttext.model\"\n",
        "model_path = home_path + 'data/word2vec.model'\n",
        "dimension = 768\n",
        "window = 5\n",
        "min_count = 2\n",
        "iter = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIsQlA5wCK4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "train_embedding(model_path, tokenized_texts, dimension, window, min_count,iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EtJM3iUZvmR7",
        "colab": {}
      },
      "source": [
        "def load_static_embeddings(f_path):\n",
        "  with open(f_path) as f:\n",
        "    data = json.load(f)\n",
        "  return data\n",
        "def pad_char_embedd(char_value):\n",
        "  #Static embeddings for paddings downloaded from official XLNet model\n",
        "  static_embeddings = load_static_embeddings(\"data/special_tokens_emb.json\")\n",
        "  if char_value == \"PAD\":\n",
        "    embedding = static_embeddings[\"pad\"]\n",
        "  #Embeddings for Unknown token\n",
        "  elif char_value == \"UNK\":\n",
        "    embedding = static_embeddings['unk']\n",
        "  #Embeddings for token sep\n",
        "  elif char_value == \"SEP\":\n",
        "    embedding = static_embeddings[\"sep\"]\n",
        "  #Embeddings for token cls\n",
        "  elif char_value == \"CLS\":\n",
        "    embedding = static_embeddings[\"cls\"]\n",
        "  #Embeddings for token mask\n",
        "  elif char_value == \"MASK\":\n",
        "    embedding = static_embeddings[\"mask\"]\n",
        "  #Embeddings for token eop\n",
        "  elif char_value == \"EOP\":\n",
        "    embedding = static_embeddings[\"eop\"]\n",
        "  #Embeddings for token eod\n",
        "  elif char_value == \"EOD\":\n",
        "    embedding = static_embeddings[\"eod\"]\n",
        "  del static_embeddings \n",
        "  return embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aT75LiiYI6vx",
        "colab": {}
      },
      "source": [
        "def get_initial_embeddings_for_xlnet(model):\n",
        "    embeddings = []\n",
        "    valid,sep,unk,pad,cls,eod,eop,mask,other = 0,0,0,0,0,0,0,0,0\n",
        "    for i in range(32000):\n",
        "        token = tokenizer.convert_ids_to_tokens(i)\n",
        "        try:\n",
        "            token_embedd = model.wv[token]\n",
        "            valid += 1\n",
        "        except:\n",
        "            if token in [\"<SEP>\",\"[sep]\",\"<sep>\",\"[SEP]\"]:\n",
        "                token_embedd = pad_char_embedd(\"SEP\")\n",
        "                sep += 1\n",
        "            elif token in [\"<CLS>\",\"[cls]\",\"<cls>\",\"[CLS]\"]:\n",
        "                token_embedd = pad_char_embedd(\"CLS\")\n",
        "                cls += 1\n",
        "            elif token in [\"<UNK>\",\"[unk]\",\"<unk>\",\"[UNK]\"]:\n",
        "                token_embedd = pad_char_embedd(\"UNK\")\n",
        "                unk += 1\n",
        "            elif token in [\"<PAD>\",\"[pad]\",\"<pad>\",\"[PAD]\"]:\n",
        "                token_embedd = pad_char_embedd(\"PAD\")\n",
        "                pad += 1\n",
        "            elif token in [\"<EOP>\",\"[eop]\",\"<eop>\",\"[EOP]\"]:\n",
        "                token_embedd = pad_char_embedd(\"EOP\")\n",
        "                eop += 1\n",
        "            elif token in [\"<EOD>\",\"[eod]\",\"<eod>\",\"[EOD]\"]:\n",
        "                token_embedd = pad_char_embedd(\"EOD\")\n",
        "                eod += 1\n",
        "            elif token in [\"<MASK>\",\"[mask]\",\"<mask>\",\"[MASK]\"]:\n",
        "                token_embedd = pad_char_embedd(\"MASK\")\n",
        "                mask += 1\n",
        "            else:\n",
        "                token_embedd =  np.random.rand(768,)\n",
        "                other += 1\n",
        "        embeddings.append(token_embedd) \n",
        "    counters = [valid,sep,unk,pad,cls,eod,eop,mask,other]\n",
        "    return embeddings, counters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPRwHXCeCPeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vecmodel = Word2Vec.load(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qS8x4VmJJOEz",
        "colab": {}
      },
      "source": [
        "i_embeddings,counters = get_initial_embeddings_for_xlnet(word2vecmodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w-aXH27FJV4y",
        "colab": {}
      },
      "source": [
        "np.savetxt(\"data/initial_embeddings_for_xlnet.txt\",np.array(i_embeddings))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x2iupkVH4dZT",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}