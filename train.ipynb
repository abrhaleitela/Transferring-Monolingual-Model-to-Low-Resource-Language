{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipmnez6Ms0kF",
        "colab_type": "code",
        "outputId": "2849bbb8-ad0b-444d-89ce-6c59820e68e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#If you want to use Google Colab GPU\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('GPU found at: ', device_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "print(\"Device connected: \", torch.cuda.get_device_name(0))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU found at:  /device:GPU:0\n",
            "Device connected:  Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NuF4uFUEc7j",
        "colab_type": "code",
        "outputId": "92f0718e-30ae-42ce-a621-a024478f688b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "#Install transformers if you dont have it installed already\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82tLuxR2ui35",
        "colab_type": "code",
        "outputId": "e56e8f33-b335-4066-d108-fd6824c88ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Import necessary libs\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import sentencepiece as spm\n",
        "from transformers import XLNetModel,XLNetConfig, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from transformers import AdamW\n",
        "import random\n",
        "import nltk\n",
        "from gensim.models import Word2Vec,word2vec\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, precision_score,recall_score,f1_score\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPdnL-IFuJ5u",
        "colab_type": "code",
        "outputId": "bc2bea5a-d4cd-46f7-e45d-435a2c2690f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# If you want to mount your Google Drive to access dataset is in your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8-X3MePuQdk",
        "colab_type": "code",
        "outputId": "1327ed4c-1ac3-42e7-c3d0-ca1d9091982f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "#Load dataset to pandas DataFrame\n",
        "df = pd.read_csv('path/to/train.csv', delimiter= \";\",header = None)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ኣቲ ከልቢ በጃኪ አባ ምሓርና ኣይትሓፍርን ይዀነለይ ከ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ኣኤባሳኣ ጀኤገናኣ ታኣውዓተ ሕያተኤ ጋሌኤ ኤርትራኣውተ ተዝርበይው ዝላከኤ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>እንቋዕ ብዳሓን መጻኩም እንዳ ጅሮም ቖቃሕ ንኩሉኩም ተዋሳእቲ ኣብዚኣ ደቂ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>ዋውውውው ፋሚሊ ጆን ጽቡቕ ስራሕ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>ይሁዳውያን የዕርኩቱ ሸጣዉያን ስርዓቱ ጋኒናት መንፈሶም</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0                                                  1\n",
              "0  0                 ኣቲ ከልቢ በጃኪ አባ ምሓርና ኣይትሓፍርን ይዀነለይ ከ\n",
              "1  1  ኣኤባሳኣ ጀኤገናኣ ታኣውዓተ ሕያተኤ ጋሌኤ ኤርትራኣውተ ተዝርበይው ዝላከኤ...\n",
              "2  1  እንቋዕ ብዳሓን መጻኩም እንዳ ጅሮም ቖቃሕ ንኩሉኩም ተዋሳእቲ ኣብዚኣ ደቂ...\n",
              "3  1                               ዋውውውው ፋሚሊ ጆን ጽቡቕ ስራሕ\n",
              "4  0                 ይሁዳውያን የዕርኩቱ ሸጣዉያን ስርዓቱ ጋኒናት መንፈሶም"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfV9ez2VueEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change datatype of our label\n",
        "df[0] = (df[0]).astype(int)\n",
        "#Shuffles data \n",
        "df = shuffle(df)\n",
        "sentences = df[1].values\n",
        "labels = df[0].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChtAJZ-BvCr8",
        "colab_type": "code",
        "outputId": "94408403-308d-4e92-a0f9-1ff6c42df77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#Add two tokens used in XLNet pretraining: [SEP] uses to separate sentences and [CLS] for classfication tasks\n",
        "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
        "\n",
        "#Load Tigrinya tokenizer \n",
        "tokenizer = XLNetTokenizer.from_pretrained('path/to/target-Tigrinya/language/tokenizer.model')\n",
        "#Tokenize input sentences using a new Tigrinya sentencePiece model\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence of Tigrinya comment:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling XLNetTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence of Tigrinya comment:\n",
            "['▁ኣነ', '▁ወዲ', '▁ስዉእ', '▁እየ', '▁ሓደ', '▁ጻ', 'ን', 'ታን', '▁በዓል', '▁ነርኪ', '▁ብጾት', '▁ኣቦይ', '▁ክብለኩም', '▁ብፍጹም', '▁ኣይ', 'ደፍር', 'ን', '▁ምስ', '▁ደቀይ', 'ን', '▁ሰበይተይ', '▁ኣብ', '▁ስደት', '▁ኣታ', '▁ኮመን', 'ተይ', '▁ተንብብ', '▁ዘለካ', '▁ሃገራዊ', '▁ዓቕምና', '▁ፈጺምና', '▁ኢና', '▁ኣነ', 'ን', '▁ሰበይተይ', 'ን', '▁ደቀይ', '▁ግን', '▁ኣበ', 'ደን', '▁ኣውጺአ', 'ዮም', '▁ኣለኩ', '▁ከምቶም', '▁ሓለፍትን', '▁ደገ', '▁ዘለኩም', 'ን', '▁ደቅ', 'ኩም', '▁ሓቢ', 'እኩም', '▁ዘለኩም', '▁ሃገር', '▁ን', 'በይ', 'ን', 'ና', '▁ኣይት', 'ምልከተ', 'ናን', '▁እያ', '▁ኣይት', 'ዕ', 'ምጽ', 'ን', '▁ህግደፍ', '▁ውጉኣት', '▁ኣቦታትና', '▁ኮንኩም', '▁ኣይትፈልጡን', '▁ትዕግስቲ', '▁ዶ', '▁ኣቢልኪ', '▁ሳላ', 'ሳ', '▁ዓመት', '▁ብሰብ', 'ዓ', '▁ኢልኩም', '▁ስልጣን', '▁ሒዝኩም', '▁ሰላሳ', '▁ዓመት', '▁ገዚእ', 'ኩምና', '▁ይኣክለኩም', '▁ይኣክል', '▁ተወሳኪ', '▁እየ', '▁ዘረባ', '▁ኣይን', 'ፈቱ', 'ን', '▁ድያ', '▁ዝበለት', '▁እወ', '▁ሓቅ', 'ኪ', '▁ሓቅነት', '▁ዘይብልኩም', '▁ከመይ', '▁ክትዛረቡ', '▁እስኩም', '▁ትፈትው', 'ዎ', '▁ምእሳር', 'ን', '▁ምቅታል', 'ን', '▁ካብ', '▁ዓዲ', '▁ንዝ', 'ቃወመ', 'ኩም', '▁አ', 'ሲ', 'ርካ', '▁ክሰርሕ', '▁ክነ', 'ግድ', '▁ዝደለየ', '▁ቦታ', '▁ምዕጻው', '▁ባዕልኩም', '▁ነጋዶ', '▁ባዕልኩም', '▁', 'ፈረድ', 'ቲ', '▁ሓ', 'ገግ', 'ቲ', '▁ግን', '▁ዘለኽ', 'ስ', '▁ኣዝዩ', '▁ፍርሒ', '▁ኣለኪ', '▁ፎዝያ', '▁ሓሽም', '▁ዜ', 'ፍቅ', 'ሩ', '▁ንመን', '▁ፈቲ', 'ሕ', 'ክሉ', '▁እዞም', '▁ኩሉ', 'ም', '▁እሱራት', '▁ንምንታይ', '▁ናባ', 'ኪ', '▁ተሓ', 'ቲ', 'ቶም', '▁ማር', 'ሞ', '▁ኢኪ', '▁ወ', 'ደሓን', 'ኪ', '▁', '[SEP]', '▁', '[CLS]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNss2mt6vdXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length  (The longer the better if you have enough RAM and have a longer sentences)\n",
        "MAX_LEN = 180\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moLStp3RwieB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad the input tokens to match the MAX length set\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "attention_masks = []\n",
        "# Mask: 1s for each token and 0s for padding and create attention masks\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjskw-OgxcG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split training data into Train/Validation (80%/20%)\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4arrsBr8xtdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create torch tensors \n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg0OrnFU0DoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model hyperparameters\n",
        "LEARNING_RATE = 2e-5\n",
        "batch_size = 32\n",
        "epochs = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uAfj2Mux6Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates torch dataloaders for training and validation sets \n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX_NaZk0yr-q",
        "colab_type": "code",
        "outputId": "e133e84d-31d1-4c12-be52-54369760ab2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loading pre-trained English XLNet model with 2 output layers \n",
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
        "model.cuda()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE9XtbNNzF4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get default model parameters and set weight decay for the following params\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "param_optimizer = list(model.named_parameters())\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# Creates an optimizer with a learning rate of LEARNING_RATE\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTXvtoAV0QBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate F1-Score \n",
        "def get_f1score(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(pred_flat, labels_flat)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuhzSrdu1IHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Word2Vec token embeddings trained for target language (Note that the txt file(token embeddings) can be created using Word2Vec_token_embeddings_for_xlnet.ipynb )\n",
        "# And initialize our XLNet model with the target language embeddings\n",
        "i_embeddings = np.loadtxt('path/to/learned/initial_embeddings.txt', dtype=float)\n",
        "i_embeddings = torch.nn.Embedding.from_pretrained(torch.FloatTensor(i_embeddings)).to(device)\n",
        "model.set_input_embeddings(i_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_mrDyki1q8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze the embedding layer to prevent from updating the pre-trained weights \n",
        "for name, param in model.named_parameters():                  \n",
        "    if name.startswith('transformer.word_embedding'):\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "      param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TzaIQT69thY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main training, loss and accuracy \n",
        "train_loss_set, total_train_loss = [],[]\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Train\n",
        "  model.train()\n",
        "\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  avg_loss = 0\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    input_ids, b_input_mask, b_labels = batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(input_ids = input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss = outputs[0]\n",
        "    logits = outputs[1]\n",
        "    avg_loss += loss.item()\n",
        "    total_train_loss.append(loss.item())\n",
        "    \n",
        "    if step % 1000 == 0 and step != 0:\n",
        "      train_loss_set.append(avg_loss/1000)\n",
        "      avg_loss = 0\n",
        "    \n",
        "    train_loss_set.append(loss.item())    \n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  # Validate\n",
        "  model.eval()\n",
        "\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    input_ids, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids = input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = output[0]\n",
        "    \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = get_f1score(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"\\n Validation Accuracy: \", eval_accuracy/nb_eval_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1rsUktm5WOS",
        "colab_type": "text"
      },
      "source": [
        "**Note that:** if you don't want to store the fine-tuned model before evualating in test set please copy the content of test.ipynb and evaulate the model with out saving it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIhgVE0X5h-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model \n",
        "model_save = '/path/'\n",
        "model.save_pretrained(model_save)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}