{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "extract_Tigrinya_comments.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "26GpoPiyAVwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        " #Goal of this implementation is to split an input file of large curpos which contains multiple language content into three files\n",
        "     Tig_Emoji(A file which has both Tigrinya comments and also emoji)\n",
        "     Tig_pure(pure Tigrinya comments without any emojis)\n",
        "     Garbage(A file which has multiple languages(other than Tigrinya) content)\n",
        "\"\"\"\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "import emoji\n",
        "import random\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sTn4MIgAK4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store(comments, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(comments))\n",
        "        \n",
        "def store_as_text(comments, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(comments)\n",
        "        \n",
        "def read_all_comments(path):\n",
        "    \"\"\"Read data from given txt file and return all comments as an array\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = file.readlines()\n",
        "    datas = [comment for comment in data if not (comment == \"\" or comment == \" \")]\n",
        "    \n",
        "    return [comment.replace('\\n','') for comment in datas]\n",
        "\n",
        "def read_plain_comments(path):\n",
        "    \"\"\"Read data from given txt file and return all comments as an array\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = file.readlines()\n",
        "    return data\n",
        "def load_punctuations():\n",
        "    punctuations = ['፡', '።', '፣', '፤', '፥', '፦', '፧', '፨']\n",
        "    for pun in string.punctuation:\n",
        "        punctuations.append(pun)\n",
        "    return punctuations\n",
        "\n",
        "def remove_punctuations(data, punctuations):\n",
        "    newdata = data\n",
        "    for punctuation in punctuations:\n",
        "        newdata = newdata.replace(punctuation,' ')\n",
        "    return newdata\n",
        "\n",
        "def remove_emoji(text):\n",
        "    return emoji.get_emoji_regexp().sub(u'', text)\n",
        "\n",
        "def read_all_emojis(path):\n",
        "    emojis = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = file.readlines()\n",
        "    for line in data:\n",
        "        if \":\" in line:\n",
        "            emojis.append(line.split(':')[1].replace('\\n',''))\n",
        "    #emojis = [emoji.replace('\\n','') for emoji in data]\n",
        "    return emojis\n",
        "\n",
        "\"\"\" Remove amharic comments from tigrinya\"\"\"\n",
        "def remove_amharic(myarr, amharic_letters):\n",
        "    tig_exc_amh = []\n",
        "    for comment in myarr:\n",
        "        isAmharic = False\n",
        "        for ltr in amharic_letters:\n",
        "            if ltr in comment:\n",
        "                isAmharic = True\n",
        "                break\n",
        "        if not isAmharic:\n",
        "            tig_exc_amh.append(comment)\n",
        "    return tig_exc_amh\n",
        "\n",
        "def isAmharic(comment, amharic_letters):\n",
        "    for ltr in amharic_letters:\n",
        "        if ltr in comment:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpnfVJscAK4N",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1a81439-75bb-42dc-a029-c67ce5db9c26"
      },
      "source": [
        "alphas = list(string.ascii_letters)\n",
        "HOME_PATH = \"PATH TO DIRECTORY THAT CONTAINS ALL THE txt FILES TO BE PROCESSED\"\n",
        "EMOJI_FILE = \"data\\emojilist.txt\"\n",
        "amharic_letters = ['ሇ','ሗ','ሧ','ሷ','ቇ','ቧ','ቷ','ኇ','ኗ','ኧ','ዧ','ዷ','ጇ','ጧ','ጷ\t','ፇ','ፗ','ፘ\t','ፙ\t','ፚ','ፏ','ጿ','ጯ','ጟ','ጏ','ዿ','ዯ','ዟ','ዏ','ኯ','ኟ','ቿ','ቯ','ሿ','ሯ','ሟ','ሏ','ⶇ','ⶶ']\n",
        "\n",
        "print(\"Checking and verifying languages...\\n\")\n",
        "\n",
        "for filename in os.listdir(HOME_PATH):\n",
        "    if filename.endswith(\".txt\"):    \n",
        "        TIG_PURE_OUTPUT_FILE = HOME_PATH + \"Pure Tig\\\\\" + filename.split(\".\")[0] + \"_Pure_Tig.txt\"\n",
        "        TIG_Emoji_OUTPUT_FILE = HOME_PATH + \"Tig Emoji\\\\\" + filename.split(\".\")[0] + \"_Tig_Emoji.txt\"\n",
        "        NONTIG_OUTPUT_FILE = HOME_PATH + \"Other Lang\\\\\" + filename.split(\".\")[0] + \"_Non_Tig.txt\"\n",
        "        \n",
        "        INPUT_FILE = HOME_PATH + filename\n",
        "\n",
        "        comments = read_all_comments(INPUT_FILE)\n",
        "        emojis = read_all_emojis(EMOJI_FILE)\n",
        "        Tig_pure_comments = []\n",
        "        Tig_emoji_comments = []\n",
        "        NonTig_comments = []\n",
        "        \n",
        "        print(\"Loading and verifying comments in\",filename)\n",
        "        \n",
        "        for comment in comments:\n",
        "            eng_flag = any(alpha in comment for alpha in alphas)\n",
        "            emoji_flag = any(emoji in comment for emoji in emojis)\n",
        "            arabic_flag = re.findall(r'[\\u0600-\\u06FF]+', comment)\n",
        "            if not eng_flag and not emoji_flag and len(arabic_flag) == 0 and not isAmharic(comment,amharic_letters):\n",
        "                #comment is Tigrinya does not have any emoji     \n",
        "                Tig_pure_comments.append(comment) \n",
        "            elif not eng_flag and emoji_flag and len(arabic_flag) == 0 and not isAmharic(comment, amharic_letters):\n",
        "                #comment is Tigrinya but contains emoji\n",
        "                Tig_emoji_comments.append(comment)\n",
        "            else:\n",
        "                #comment is not Tigrinya\n",
        "                NonTig_comments.append(comment)\n",
        "            #store them to three files\n",
        "            #MOVE_TO = HOME_PATH + \"All Comments of Channel\\\\Done splitting\\\\\" + filename\n",
        "            #os.rename(INPUT_FILE,MOVE_TO)\n",
        "        print(\"Saving 3 extracted from\", filename, \"\\b...\")\n",
        "        store(Tig_emoji_comments, TIG_Emoji_OUTPUT_FILE)\n",
        "        store(Tig_pure_comments,TIG_PURE_OUTPUT_FILE)\n",
        "        store(NonTig_comments,NONTIG_OUTPUT_FILE)\n",
        "        print(\"Saved...\")\n",
        "print(\"All files check and verified successfully...\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking and verifying languages...\n",
            "\n",
            "Loading and verifying comments in only_tiamru_videos.txt\n",
            "Saving 3 extracted from only_tiamru_videos.txt...\n",
            "Saved...\n",
            "All files check and verified successfully...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}